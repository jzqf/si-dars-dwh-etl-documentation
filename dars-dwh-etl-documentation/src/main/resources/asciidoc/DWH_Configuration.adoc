//------------------------------------------------------------------------------
:author:    Jeffrey Zelt
:email:     jeffrey.zelt@q-free.com
:revdate:   Oct 4, 2017
:revnumber: 0.5

:customer:               DARS

:psa_dbname_full:        Persistent Staging Area database
:psa_dbname_abbrev:      PSA DB

:etl_dbname_full:        ETL database
:etl_dbname_abbrev:      ETL DB

:logging_dbname_full:    Logging database
:logging_dbname_abbrev:  Logging DB

:obo_opr_dbname_full:    OBO operational database
:obo_opr_dbname_abbrev:  OBO OPR DB

:odms_opr_dbname_full:   ODMS operational database
:odms_opr_dbname_abbrev: ODMS OPR DB

:app_root_dir:       /opt/qfree
:app_name:           dwh-etl
:app_user:           dwh_etl
:app_group:          dwh_etl
:bin_dir:            bin
:conf_dir:           conf
:lib_dir:            lib
:log_dir:            logs
:template_dir:       templates
:pdi_config_dir:     pdi_config
:pdi_repository_dir: pdi_repository

:customer_psa_dbname_full:   {customer} {psa_dbname_full}
:customer_psa_dbname_abbrev: {customer} {psa_dbname_abbrev}
//------------------------------------------------------------------------------


// Change this title to be more general if we maintain more than one target DB!!!
= Configuration of the {customer_psa_dbname_full} ETL framework


== Summary

After all software packages have been installed and the relevant databases have 
been set up, there are configuration tasks to perform that are specific to
the customer's installation. This includes:

* Choosing values for Q-Free configuration parameters for:
+
--
. Database connections
. Logging data retention
. E-mail notification of ETL job success/failure

See the "<<qfree_config_params-section>>" section below.
--
//. TDP star schema data mart setup
//. Postgresql `psql` utility location

* Choosing values for customer (end-user) configuration parameters for:
+
--
. SMTP server
. E-mail notification of ETL job success/failure

See the "<<customer_config_params-section>>" section below.
--

* Setting environment variables for the "{app_user}" Linux account under which
all ETL jobs run. See the "<<environment_variables-section>>" section below.
* Scheduling ETL jobs. See the "<<scheduling_and_running_jobs-section>>" section
below
* Customization of logging details. See the "<<logging-section>>"" section 
below.
* Populating tables in the {etl_dbname_full} that provide metadata to the ETL 
jobs. This metadata controls the behaviour of the algorithms that are 
implemented in these jobs. See the "<<populating_etl_database-section>>" 
section below.


[id="qfree_config_params-section"]
== Q-Free configuration parameters

"Q-Free" configuration parameters are parameters whose values must be chosen by
Q-Free. These are values should also not be visible to the customer, as they may 
reveal details that are either confidential or details that are irrelevant to 
the needs of the customer.

These parameters are stored in a file named `dwh-qfree.properties`, which is
stored here:
 
// This is needed for Asciidoc attributes to be evaluated in a literal block:
[subs="attributes"]
 {app_root_dir}/{app_user}/{conf_dir}/dwh-qfree.properties

// It also works with "...." literal blocks:
//[subs="attributes"]
//....
//{app_root_dir}/{app_user}/{conf_dir}/dwh-qfree.properties
//....

Values must be set appropriately for all parameters (referred to as "properties"
in Java) in this file before any ETL script is executed for loading or
updating the {psa_dbname_abbrev}.

The following table lists the parameters in `dwh-qfree.properties`, together 
with a representative
or default value and a short description of what the parameter value 
represents. Do not assume that the example values provided here will match your
environment. 

.Q-Free configuration parameters (dwh-qfree.properties)
|===
|Name |Example value |Description

3+|

3+|__{psa_dbname_full} ({psa_dbname_abbrev}). It may be appropriate to provide the customer
with the credentials of a PostgreSQL role that has SELECT access to the tables in the 
SQL schemas of this database. This database can be the same as the {etl_dbname_full} 
that is configured below, provided that unique SQL schema names are specified.
However, administration will be simpler if different databases are used, 
potentially installed on different hosts as well.__

|*QF_PSA_DB_HOST* |localhost |Hostname of the server that hosts the {psa_dbname_abbrev}.
|*QF_PSA_DB_PORT* |5432 |TCP port for connections to the {psa_dbname_abbrev}.
|*QF_PSA_DB_DATABASE* |dars_psa_db |Name of the {psa_dbname_abbrev}.
|*QF_PSA_DB_USERNAME* |etl |Username for for connections to the {psa_dbname_abbrev}.
|*QF_PSA_DB_PASSWORD* |etl |Password for for connections to the {psa_dbname_abbrev}.

//3+|
//
//3+|__TDP data mart database. The customer should be provided with the credentials 
//of a PostgreSQL role that has SELECT access to the tables in the SQL schema of 
//this database.  This database can be the same as the {psa_dbname_full}
//that is configured above, provided that a unique SQL schema is specified here.__
//
//|*DWH_TDP_DMA_DB_HOST* |localhost |Hostname of the server that hosts the TDP data mart database.
//|*DWH_TDP_DMA_DB_PORT* |5432 |TCP port for connections to the TDP data mart database.
//|*DWH_TDP_DMA_DB_DATABASE* |dars_dma_db |Name of the TDP data mart database.
//|*DWH_TDP_DMA_DB_USERNAME* |etl |Username for for connections to the TDP data mart database.
//|*DWH_TDP_DMA_DB_PASSWORD* |etl |Password for for connections to the TDP data mart database.
//|*DWH_TDP_DMA_DB_SCHEMA* |dma |Name of SQL schema containing the TDP data mart tables. These 
//tables will represent facts and dimensions. The customer will be given SELECT
//permission for the tables in this schema. The data in these tables can be used
//for generating reports, performing analysis, etc.

3+|

3+|__{obo_opr_dbname_full} ({obo_opr_dbname_abbrev}). This database provides source data for the target
{psa_dbname_abbrev}. The customer should *not* be provided with the 
credentials of a PostgreSQL role to access objects in this database. Data from 
this database that the customer has legitimate need to see should be exposed,
instead, by the {psa_dbname_abbrev}.__

|*QF_OBO_OPR_DB_HOST* |<someserver>.q-free.com |Hostname of the server that hosts the {obo_opr_dbname_abbrev}.
|*QF_OBO_OPR_DB_PORT* |5432 |TCP port for connections to the {obo_opr_dbname_abbrev}.
|*QF_OBO_OPR_DB_DATABASE* |obo_opr |Name of the {obo_opr_dbname_abbrev}.
|*QF_OBO_OPR_DB_USERNAME* |qfree_obo |Username for for connections to the {obo_opr_dbname_abbrev}.
|*QF_OBO_OPR_DB_PASSWORD* |_<somepassword>_ |Password for for connections to the {obo_opr_dbname_abbrev}.

3+|

3+|__{odms_opr_dbname_full} ({odms_opr_dbname_abbrev}). This database provides source data for the target
{psa_dbname_abbrev}. The customer should *not* be provided with the 
credentials of a PostgreSQL role to access objects in this database. Data from 
this database that the customer has legitimate need to see should be exposed,
instead, by the {psa_dbname_abbrev}__

|*QF_ODMS_OPR_DB_HOST* |<someserver>.q-free.com |Hostname of the server that hosts the {odms_opr_dbname_abbrev}.
|*QF_ODMS_OPR_DB_PORT* |5432 |TCP port for connections to the {odms_opr_dbname_abbrev}.
|*QF_ODMS_OPR_DB_DATABASE* |odms_opr |Name of the {odms_opr_dbname_abbrev}.
|*QF_ODMS_OPR_DB_USERNAME* |qfree_odms |Username for for connections to the {odms_opr_dbname_abbrev}.
|*QF_ODMS_OPR_DB_PASSWORD* |_<somepassword>_ |Password for for connections to the {odms_opr_dbname_abbrev}.

3+|

3+|__{etl_dbname_full}. The tables in this database are used to configure the 
behaviour of the ETL jobs that maintain the {psa_dbname_abbrev}. See section
"<<populating_etl_database-section>>" below for details on how to populate this
database.
The customer should *not* be  given access to the tables in this database. 
This database can be the same as the {psa_dbname_abbrev} 
that is configured above, provided that a unique SQL schema is specified here.
However, administration will be simpler if different databases are used, 
potentially installed on different hosts as well.__

|*QF_ETL_DB_HOST* |localhost |Hostname of the server that hosts the {etl_dbname_full}.
|*QF_ETL_DB_PORT* |5432 |TCP port for connections to the {etl_dbname_full}.
|*QF_ETL_DB_DATABASE* |dars_dwh_etl_db |Name of the {etl_dbname_full}.
|*QF_ETL_DB_USERNAME* |etl |Username for for connections to the {etl_dbname_full}.
|*QF_ETL_DB_PASSWORD* |etl |Password for for connections to the {etl_dbname_full}.
|*QF_ETL_DB_SCHEMA* |etl |Name of SQL schema containing the ETL support tables

3+|

3+|__{logging_dbname_full}. All logging tables are placed in the "etl" schema; this is 
not currently configurable. These tables used to support logging for the ETL 
operations. The customer should not be given access to these tables. This 
database can be the same as the other databases that are configured above.__

|*QF_LOGGING_DB_HOST* |localhost |Hostname of the server that hosts the {logging_dbname_abbrev}.
|*QF_LOGGING_DB_PORT* |5432 |TCP port for connections to the {logging_dbname_abbrev}.
|*QF_LOGGING_DB_DATABASE* |dars_dwh_etl_db |Name of the {logging_dbname_abbrev}.
|*QF_LOGGING_DB_USERNAME* |etl |Username for for connections to the {logging_dbname_abbrev}.
|*QF_LOGGING_DB_PASSWORD* |etl |Password for for connections to the {logging_dbname_abbrev}.

3+|

3+|__Number of days to retain rows inserted into logging tables:__

|[small]#*QF_LOG_JOB_TIMEOUT_IN_DAYS*#|90 |This table logs data for PDI 
jobs that have been executed by the ETL scripts.

|[small]#*QF_LOG_JOBENTRY_TIMEOUT_IN_DAYS*#|30 |This table logs data for PDI job 
entries.

|[small]#*QF_LOG_CHANNEL_TIMEOUT_IN_DAYS*#|30 |This table logs PDI logging channel 
data. This is used to connect log entries between jobs, job entries, 
transformations, transformation steps and transformation metrics.

|[small]#*QF_LOG_TRANS_TIMEOUT_IN_DAYS*#|90 |This table logs data for PDI 
transformations that have been executed by the ETL scripts.

|[small]#*QF_LOG_TRANSSTEP_TIMEOUT_IN_DAYS*#|30 |This table logs data for PDI 
transformation steps.

|[small]#*QF_LOG_TRANSPERF_TIMEOUT_IN_DAYS*#|30 |This table logs
transformation performance data.

|[small]#*QF_LOG_TRANSMETRICS_TIMEOUT_IN_DAYS*#|30 |This table logs
transformation metrics data.

3+|

3+|__E-mail details for messages sent to a Q-Free recipient:__

|[small]#*QF_MAIL_MSG_QFREE_ADDRESS_TO*#| someone@q-free.com|E-mail address, 
comma-separate list of addresses, or a distribution list to send the e-mail to.
If this is empty, no e-mail will be sent.

|[small]#*QF_MAIL_MSG_QFREE_ADDRESS_CC*#| |E-mail address, comma-separate list of 
addresses, or a distribution list to send a carbon copy of the e-mail to.

|[small]#*QF_MAIL_MSG_QFREE_ADDRESS_BCC*#| |E-mail address, comma-separate list of 
addresses, or a distribution list to send a blind carbon copy of the e-mail to. 

|[small]#*QF_MAIL_MSG_QFREE_SENDER_NAME*#|Q-Free ETL process |The name of the 
person or account that the e-mail should appear to come from.

|[small]#*QF_MAIL_MSG_QFREE_SENDER_ADDRESS*#|etl@q-free.com |The e-mail address of the 
person or account that the e-mail should appear to come from. 

|[small]#*QF_MAIL_MSG_QFREE_REPLY_TO_ADDRESS*#|some-service-address@q-free.com |The e-mail 
address that a recipient should use to reply to the e-mail. 

|[small]#*QF_MAIL_MSG_QFREE_CONTACT_PERSON*#| |The name of the person to contact regarding 
the content of the e-mail sent. 

|[small]#*QF_MAIL_MSG_QFREE_CONTACT_PHONE*#| |The phone number of the contact person.

|[small]#*QF_MAIL_MSG_QFREE_SUBJECT_SUCCESS*#|Successful execution: ETL job "${PARAM_JOB_NAME}"
|Template for the e-mail subject for the notification of a successfully executed PDI job.

|[small]#*QF_MAIL_MSG_QFREE_SUBJECT_FAILURE*#|FAILED EXECUTION: ETL job "${PARAM_JOB_NAME}"
|Template for the e-mail subject for the notification of a failed PDI job.

|[small]#*QF_MAIL_MSG_QFREE_BODY_SUCCESS*#|The job "${PARAM_JOB_NAME}" executed successfully.${line.separator}
Job ID = ${PARAM_JOB_BATCH_ID}
|Template for the e-mail body for the notification of a successfully executed PDI job.

|[small]#*QF_MAIL_MSG_QFREE_BODY_FAILURE*#|[small]#The job "${PARAM_JOB_NAME}" failed.${line.separator}
Job ID = ${PARAM_JOB_BATCH_ID}${line.separator}
${line.separator}
${QF_MAIL_MSG_QFREE_BODY_FAILURE_EXTRA_INFO}#
|Template for the e-mail body for the notification of a failed PDI job.
//
//3+|
//
//3+|__TDP star schema data mart setup:__
//
//|*DWH_TDP_DMA_DIM_DATE_YEAR_MIN* |2017 |The date dimension will start on January 1 of this year.
//|*DWH_TDP_DMA_DIM_DATE_YEAR_MAX* |2026 |The date dimension will end on December 31 of this year.
//
//|[small]#*DWH_TDP_DMA_DIM_TIME_RESOLUTION_SECONDS*# |1 |The resolution in seconds of the 
//time dimension. This must either divide exactly into the the number of seconds 
//in a minute (60) or be a multiple of 60, and at the same time it must either 
//divide exactly into the the number of seconds in an hour (3600) or be a multiple 
//of 3600.
//
//|*DWH_TDP_DMA_LOCALE_LANGUAGE_CODE* |en |The two-letter
//http://www.loc.gov/standards/iso639-2/php/code_list.php[ISO 639-1] language code 
//to use with the data mart. This is used together with the country code to 
//specify a locale that is used to generate locale-specific strings, e.g., days 
//names, month names, etc. Examples are: "*nb*" (Norwegian Bokmål), "*en*" 
//(English), etc.
//
//|*DWH_TDP_DMA_LOCALE_COUNTRY_CODE* |us |The two-letter 
//https://www.iso.org/obp/ui/[ISO 3166-1] country code to use
//with the data mart. This is used together with the language code to specify a
//locale that is used to generate locale-specific strings, e.g., days names, month
//names, etc. Examples are: "*no*" (Norway), "*us*" (United States of America), 
//etc.

//3+|
//
//3+|__Miscellaneous:__
//
//|*DWH_POSTGRESQL_BULK_LOADER_PSQL_PATH* |/usr/bin/psql |The absolute path to the 
//PostgreSQL "psql" tool on the local machine. This is used by the PostgreSQL Bulk 
//Loader transformation step.
|===

=== Updating the master configuration file

After configuration parameters in the file dwh-qfree.properties are
modified, it is necessary to synchronize these values with the master 
configuration file that is used by PDI applications. This is performed by
an application named `merge_configuration_files.jar` that you will find here:

[subs="attributes"]
 {app_root_dir}/{app_user}/{lib_dir}/merge_configuration_files.jar

This should be executed as user "etl" from a shell as follows:

[subs="attributes"]
 $ sudo -iu etl
 $ java -jar {app_root_dir}/{app_user}/{lib_dir}/merge_configuration_files.jar

or:

[subs="attributes"]
 $ sudo -iu etl java -jar {app_root_dir}/{app_user}/{lib_dir}/merge_configuration_files.jar

This will also write the list of *all* parameter settings from the master 
configuration file (after the new parameter settings are merge in) to stdout.
This list is for information only. You can safely ignore it, but you can also 
use it to double check that the parameters that you specified have been updated
correctly in the master configuration file.

IMPORTANT: If you forget to run `merge_configuration_files.jar`, all ETL 
scripts will continue to use the old parameter values.


[id="customer_config_params-section"]
== Customer (end-user) configuration parameters

"Customer" configuration parameters are parameters whose values can potentially 
be specified by the customer (end-user). However, in practice these will 
probably be set by Q-Free.

These parameters are stored in a file named `dwh.properties`, which is
stored here:

 /etc/dwh/dwh-etl/config/dwh.properties

Values should be set appropriately for all parameters in this file before any 
ETL script is executed for loading or updating the DWH.

It is conceivable that the customer could be given access to this file so that
the customer can maintain the values themselves. For this reason, a separate
file is provided for these parameter and it is placed in a different directory
than the other configuration file, `dwh-qfree.properties`. If this is not
desired or practical, then this file can be maintained by Q-Free.

The following table lists the parameters in `dwh.properties`, together with 
a representative
or default value and a short description of what the parameter value 
represents. Do not assume that the example values provided here will match your
environment. 

.Customer configuration parameters (dwh.properties)
|===
|Name |Example value |Description

3+|

3+|__E-mail server (e.g., for delivering mail to notify of successful or failed ETL jobs):__

|*DWH_MAIL_SERVER_HOST* |smtp.company.com |Hostname, or IP address of the SMTP server to use for sending e-mail.
|*DWH_MAIL_SERVER_PORT* |25 |TCP port of the SMTP service. 
|*DWH_MAIL_SERVER_USE_AUTHENTICATION* |false |"true" if server authentication is required; "false" otherwise.
|*DWH_MAIL_SERVER_AUTHENTICATION_USER* | |The SMTP username to use if server authentication is required. 
|*DWH_MAIL_SERVER_AUTHENTICATION_PASSWORD* | |The password for the SMTP username

|*DWH_MAIL_SERVER_SECURE_CONNECTION_TYPE* | 
|Encryption protocol to use for server authentication. Must be one of "", "SSL" or "TLS".

3+|

3+|__E-mail details for messages sent to the customer:__

|*DWH_MAIL_MSG_ADDRESS_TO* | |E-mail address, 
comma-separate list of addresses, or a distribution list to send the e-mail to.
If this is empty, no e-mail will be sent.

|*DWH_MAIL_MSG_ADDRESS_CC* | |E-mail address, comma-separate list of 
addresses, or a distribution list to send a carbon copy of the e-mail to.

|*DWH_MAIL_MSG_ADDRESS_BCC* | |E-mail address, comma-separate list of 
addresses, or a distribution list to send a blind carbon copy of the e-mail to. 

|*DWH_MAIL_MSG_SENDER_NAME* |Q-Free DWH TDP ETL process |The name of the 
person or account that the e-mail should appear to come from.

|*DWH_MAIL_MSG_SENDER_ADDRESS* |dwh-etl@q-free.com |The e-mail address of the 
person or account that the e-mail should appear to come from. 

|*DWH_MAIL_MSG_REPLY_TO_ADDRESS* |some-service-address@q-free.com |The e-mail 
address that a recipient should use to reply to the e-mail. 

|*DWH_MAIL_MSG_CONTACT_PERSON* | |The name of the person to contact regarding 
the content of the e-mail sent. 

|*DWH_MAIL_MSG_CONTACT_PHONE* | |The phone number of the contact person.

|*DWH_MAIL_MSG_SUBJECT_SUCCESS* |Successful execution: DWH TDP job "${PARAM_JOB_NAME}"
|Template for the e-mail subject for the notification of a successfully executed PDI job.

|*DWH_MAIL_MSG_SUBJECT_FAILURE* |FAILED EXECUTION: DWH TDP job "${PARAM_JOB_NAME}"
|Template for the e-mail subject for the notification of a failed PDI job.

|*DWH_MAIL_MSG_BODY_SUCCESS* |The job "${PARAM_JOB_NAME}" executed successfully.
|Template for the e-mail body for the notification of a successfully executed PDI job.

|*DWH_MAIL_MSG_BODY_FAILURE* |The job "${PARAM_JOB_NAME}" failed.
|Template for the e-mail body for the notification of a failed PDI job.
|===

=== Updating the master configuration file

After configuration parameters in the file dwh.properties are
modified, it is necessary to synchronize these values with the master 
configuration file that is used by PDI applications. The procedure is 
exactly the same as that for the dwh-qfree.properties file above, i.e.,

[subs="attributes"]
 $ sudo -iu etl
 $ java -jar {app_root_dir}/{app_user}/{lib_dir}/merge_configuration_files.jar

or:

[subs="attributes"]
 $ sudo -iu etl java -jar {app_root_dir}/{app_user}/{lib_dir}/merge_configuration_files.jar

IMPORTANT: If you forget to run `merge_configuration_files.jar`, all ETL 
scripts will continue to use the old parameter values.

The application `merge_configuration_files.jar` merges, in fact, *_both_* of the
files `dwh-qfree.properties` and `dwh.properties` with the master 
configuration file. Therefore, if you need to modify both `dwh-qfree.properties` 
and `dwh.properties`, you can wait and run `merge_configuration_files.jar` just
once after these changes are made.


[id="environment_variables-section"]
== Setting environment variables for the "etl" account

The home directory of the "{app_user}" Linux account, `/home/{app_user}/` will 
contain a file named `.profile`. It will initially contain the following:

[subs="attributes"]
....
# These environment variables must be set so that the PDI ETL scripts function
# correctly for the "etl" user. These scripts are located in:
#
# {app_root_dir}/{app_user}/{bin_dir}/
#
# The scripts in this directory can either be executed manually is a shell, 
# e.g.,
#
#   $ $DWH_HOME/{bin_dir}/psa-update.sh
#
# or they can be run by cron according to a schedule that you set in the file:
#
#   /etc/cron.d/{app_name}

export DWH_HOME={app_root_dir}/{app_user}
export DWH_LOGDIR=$DWH_HOME/{log_dir}
export KETTLE_HOME=$DWH_HOME/{pdi_config_dir}
export KETTLE_JNDI_ROOT=$DWH_HOME/{pdi_config_dir}/simple_jndi

# If you encounter Java-related problems, uncomment these two lines, but make
# sure that you set JAVA_HOME to the location of *your* JRE 8 installation. 
#
# This will likely be necessary if you have multiple JRE versions installed, but
# but JRE 8 is *not* the default version. This will make it the default version
# for the "{app_user}" user.
#
#export JAVA_HOME=/usr/lib/jvm/java-8-oraclejdk-amd64
#export PATH=$JAVA_HOME/bin:$PATH
....

Be sure to read the comments that describe the environment variables `JAVA_HOME`
and `PATH`. 

After you modify `.profile`, source the file to ensure that the changes are 
used:

 $ source .profile


[id="scheduling_and_running_jobs-section"]
== Scheduling and running ETL jobs

The files in the PDI repository directory,

[subs="attributes"]
 {app_root_dir}/{app_user}/{pdi_repository_dir}/

define a series of ETL jobs for managing the {psa_dbname_abbrev}. These jobs are
executed via the shell scripts stored in the directory:

[subs="attributes"]
 {app_root_dir}/{app_user}/{bin_dir}/

These scripts are described in the document 
"Installation of the Q-Free ETL toolkit for PDI". XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Shall I care an attribute for this title?

// This comment block can replace the block following it if/when we re-enable the data mart:
////
There are currently three scripts that can be run:
....
dwh_tdp_mirrored_tables-update.sh
dwh_tdp_data_mart-update.sh
dwh_tdp_update.sh
....
////
There are currently two scripts that can be run:
....
dwh_tdp_mirrored_tables-update.sh
dwh_tdp_update.sh
....

=== Manual execution of ETL jobs

In order to manually execute one of these scripts, e.g., 
`scripts/dwh_tdp_update.sh` from the "etl" account, use:

 $ $DWH_HOME/scripts/dwh_tdp_update.sh
 
where the first `$` refers to the shell prompt character.
 
A script can also be executed from an account other than "etl", but the syntax 
is slightly different. In this case use:
 
 $ sudo -iu etl '$DWH_HOME'/scripts/dwh_tdp_update.sh

Here, the `-i` option and the single quotes around `$DWH_HOME` are essential.


=== Scheduling ETL jobs

The easiest way to schedule an ETL job is to use the Linux "cron" scheduler.

When the `dwh-etl_<version>.deb` package is first installed, a file 
named `dwh-etl` is installed here:

 /etc/cron.d/dwh-etl

This file can be used to schedule ETL scripts with the `cron` scheduler. This 
file will initially contain the following content:

....
SHELL=/bin/sh
PATH=/usr/local/bin:/bin:/usr/bin

#0 2 * * *	etl	. $HOME/.profile && $DWH_HOME/scripts/dwh_tdp_update.sh
....

The last line can be used to schedule the execution of the script
`dwh_tdp_update.sh`. The "#" character in the first 
column means that the line is commented out, i.e., it is inactive. 

To activate this line so that cron will run the script according to the 
specified schedule, just remove the "#" character. It is not necessary to
restart cron or perform any other action.

The default schedule is `0 2 * * *`, which will run the script each morning 
at 2 a.m. There is nothing special about this schedule - change it to suit your
needs.

TIP: Unschedule ETL cron jobs if they are scheduled to run while either the TDP 
archive database or the TDP data warehouse database is unavailable.


==== E-mail notification from ETL jobs

Whether ETL jobs are run manually from the command line or scheduled to run
automatically, they can be configured to deliver an email to both a Q-Free
contact as well as a customer contact to report success or failure of the job.
See the sections "<<qfree_config_params-section>>" and 
"<<customer_config_params-section>>" above for the necessary 
configuration details.

NOTE: Job notification e-mails will currently be only delivered to the Q-Free
address that is configured above. Support for also sending a notification to
the customer-configured e-mail address may be added in the future.
	
If there is a serious configuration problem, no e-mail will be sent at all. This 
case can be discovered by:

. Monitoring the log file. See "<<logging-section>>" below.
. Noticing that no e-mail has arrived when one should have arrived.

==== ETL script mutex lock

Each ETL script implements the same mutex lock to prevent the same script or
two different scripts from running simultaneously. This prevents 
simultaneous access to objects that can lead to unexpected results. It also 
prevents an unrealistic demand for server resources. The PDI jobs that are 
triggered by the shell scripts are highly multi-threaded. It is not clear that
throughput can be improved by running two or more jobs simultaneously; in fact, 
this could easily have the opposite effect.

If a script is not able to acquire the necessary lock, it will terminate
immediately. It will not wait for the lock to become free. The log file will
record this event if if occurs.

TIP: If, for some reason, the lock does not get removed automatically after a 
script runs, you can delete it manually. The lock is implemented by a directory
in the file system of the Linux host: `/tmp/dwh_tdp-lock/` .


// This comment block can replace the block following it if/when we re-enable the data mart:
////
If you do want to schedule more than one script to run on the same schedule, 
write a custom wrapper shell script that simply calls the ETL scripts in 
sequence. Schedule this wrapper script with cron. For example, to run two
scripts named `dwh_tdp_mirrored_tables-update.sh` and `dwh_tdp_data_mart-update.sh` 
on the same schedule, write a script named, e.g., `wrapper.sh` that contains:

 #!/bin/sh
 $DWH_HOME/scripts/dwh_tdp_mirrored_tables-update.sh
 $DWH_HOME/scripts/dwh_tdp_data_mart-update.sh

Note that this example is presented here only to demonstrate the recommended way
to execute two ETL scripts. The provided script `dwh_tdp_update.sh` includes the 
functionality provided by `dwh_tdp_mirrored_tables-update.sh` and 
`dwh_tdp_data_mart-update.sh`, and so it can be used instead of implementing 
this example.
////

If you do want to schedule more than one script to run on the same schedule, 
write a custom wrapper shell script that simply calls the ETL scripts in 
sequence. Schedule this wrapper script with cron. For example, to run two
scripts named `dwh_tdp_mirrored_tables-update.sh` and `dwh_tdp_another_script.sh` 
on the same schedule, write a script named, e.g., `wrapper.sh` that contains:

 #!/bin/sh
 $DWH_HOME/scripts/dwh_tdp_mirrored_tables-update.sh
 $DWH_HOME/scripts/dwh_tdp_another_script.sh


The script`wrapper.sh` must be made executable with:

 $ chmod +x wrapper.sh

Assuming that `wrapper.sh` is stored in the `scripts/` directory, it can be
scheduled with cron by placing the following line in `/etc/cron.d/dwh-etl`:

 0 2 * * *	etl	. $HOME/.profile && $DWH_HOME/scripts/wrapper.sh

Again, the schedule "`0 2 * * *`" can be changed to suit your needs. This will
prevent any lock contention between the two wrapped ETL scripts.

IMPORTANT: Do not schedule more than one ETL script on the same or similar
schedule. Because of lock contention, only one will run.


[id="logging-section"]
== Logging

By default, all ETL scripts will write logging information to a file named 
`dwh.log` in the directory:

[subs="attributes"]
 {app_root_dir}/{app_user}/logging/

When the "etl" account is first created, an environment variable named 
`DWH_LOGDIR` is defined in `~etl/.profile` with the value:

 DWH_LOGDIR=$DWH_HOME/logging

where:

 DWH_HOME=/opt/dwh/dwh-etl

This environment variable value is responsible for `dwh.log` being written to 
this directory. If you would prefer the log file `dwh.log` to be written to 
another directory, e.g., 

 /var/log/dwh-etl/

or

 /var/log/dwh/

it is only a matter of creating this directory with the appropriate permissions 
so that the "etl" account can write to it, and then updating the value of 
`DWH_LOGDIR` in `~etl/.profile`

There are also seven tables in the DWH database where very detailed logging 
information is written, but they will not be described here.

Assuming that you have logged into the "etl" account or have switched to it 
from another account using `sudo su - etl`, you can monitor the log file as it
is being written to via ETL scripts with:

 $ tail -F $DWH_LOGDIR/dwh.log

This command will still work if you change DWH_LOGDIR to specify a directory in
`/var/log/` or anywhere else.

IMPORTANT: Regardless of where the log file `dwh.log` is written, it will not be 
automatically truncated or rotated. The file will continue to grow without 
bound. If you would like to automate the rotation of this log file and to
automatically delete old log files, use a tool designed for this purpose, such
as the `logrotate` utility, which is available in most Linux distributions.


[id="populating_etl_database-section"]
== Populating the {etl_dbname_full}

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
